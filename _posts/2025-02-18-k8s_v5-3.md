---
layout:     post                    # 使用的布局（不需要改）
title:      k8s权威指南学习-3            # 标题 
subtitle:   kubernetes                     #副标题
date:       2025-02-18             # 时间
author:     wangyang                     # 作者
header-img: img/post-bg-coffee.jpeg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - k8s
---

### 核心组件的运行机制

```
某个namespace下所有pod
curl --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key --cacert /etc/kubernetes/pki/ca.crt https://10.96.0.1:443/api/v1/namespaces/default/pods
某个node节点下所有pod
curl --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key --cacert /etc/kubernetes/pki/ca.crt https://10.96.0.1:443/api/v1/nodes/k8s-node1/proxy/pods
某个pod容器
curl --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key --cacert /etc/kubernetes/pki/ca.crt https://10.96.0.1:443/api/v1/namespaces/default/pods/myweb-6cc76b867c-49k6h/proxy/demo/
某个service服务
curl --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key --cacert /etc/kubernetes/pki/ca.crt https://10.96.0.1:443/api/v1/namespaces/default/services/myweb/proxy/demo/
```

​        Kubernetes API Server 作为集群的核心， 负责集群各功能模块之间的通信，集群内的各个功能模块通过 API Server 将信息存入 etcd ，当需要获取和操作这些数据时，则通过 API Server 提供的 REST 接口 （用 GET LIST WATCH 方法 ）来实现，从而实现各模块之间的信息交互。

​        每个 Node 上的 kubelet 每隔一个时间周期就会调用一次 API Server REST 接口报告自身状态， API Server 在接收到这些信息后，会将节点状态信息更新到 etcd 。此外， kubelet 通过 API Server Watch 接口监听 Pod 信息，如果监听到新的 Pod 副本被调度绑定到本节点，则执行 Pod 对应的容器创建和启动逻辑 ；如果监听到 Pod对象被删除，则删除本节点上相应的Pod 容器；如果 监听到修改 Pod 的信息，kubelet 就会相应地修改本节点的 Pod 容器。



### 使用kubectl命令行工具管理 RBAC

----------
    kubectl create role         ////在特定命名空间内设置授权规则
    kubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods
    kubectl create role pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod
    kubectl create role foo --verb=get,list,watch --resource=replicasets.apps
    kubectl create role foo --verb=get,list,watch --resource=pods,pods/status
    kubectl create role my-component-lease-holder --verb=get,list,watch,update --resource=lease --resource-name=my-component

----------
    kubectl create clusterrole   //在集群范围内设置授权规则
    kubectl create clusterrole pod-reader --verb=get --verb=list --verb=watch --resource=pods
    kubectl create clusterrole pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod
    kubectl create clusterrole foo --verb=get,list,watch --resource=replicasets.apps
    kubectl create clusterrole foo --verb=get,list,watch --resource=pods,pods/status
    kubectl create clusterrole "foo" --verb=get --non-resource-url=/logs/*
    kubectl create clusterrole monitoring --aggregation-rule="rbac.example.com/aggregate-to-monitoring=true"

----------
    kubectl create rolebinding   //在特定命名空间进行授权，为Subject绑定role
    kubectl create rolebinding bob-admin-binding --clusterrole=admin --user=bob --namespace=acme
    kubectl create rolebinding myapp-view-binding --clusterrole==view --serviceaccount=acme:myapp --namespace=acme
    kubectl create rolebinding myappnamespace-myapp-view-binding --clusterrole=view --serviceaccount=myappnamespace:myapp --namespace=acme

----------
    kubectl create clusterrolebinding   //在集群范围内进行授权，为Subject绑定ClusterRole
    kubectl create clusterrolebinding root-cluster-admin-binding --clusterrole=cluster-admin --user=root
    kubectl create clusterrolebinding kube-proxy-binding --clusterrole=system:node-proxier --user=system:kube-proxy
    kubectl create clusterrolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp


Service Account

```
kubectl describe serviceaccounts
kubectl describe secrets <secret_name>
```



###Node管理

1、node的隔离和恢复

某个node脱离调度范围时，其上运行的Pod并不会停止运行，需要手动停止

a、kubectl replace

隔离：

    apiVersion: v1
    kind: Node
    metadata:
      name: k8s-node3
      labels:
        kubernetes.io/hostname: k8s-node3
    spec:
      unschedulable: true   //true隔离,false恢复

```
kubectl replace -f xxx.yaml
kubectl get nodes 
```

b、kubectl patch

    [root@k8s-master Chapter10]# kubectl patch node k8s-node3 -p '{"spec":{"unschedulable":true}}'
    node/k8s-node3 patched
    [root@k8s-master Chapter10]# kubectl get node
    NAME STATUS ROLES   AGEVERSION
    k8s-master   Ready  control-plane   224d   v1.28.2
    k8s-node1    Ready  <none>  224d   v1.28.2
    k8s-node2    Ready  <none>  224d   v1.28.2
    k8s-node3    Ready,SchedulingDisabled   <none>  50dv1.28.2

```
kubectl patch node k8s-node3 -p '{"spec":{"unschedulable":false}}'
```

c、kubectl cordon\uncordon

```
kubectl cordon k8s-node3   隔离
kubectl uncordon k8s-node3  恢复
```




2、更新资源对象的label

    添加label
    kubectl label pod <pod_name> key=value
    查看pod的label
    kubectl get pods -L key1,key2
    删除label
    kubectl label pod <pod_name> key-
    修改label值
    kubectl label pod <pod_name> key=value --overwrite



3、集群环境共享和隔离
不同的工作组可以在同一个Kubernetes集群中工作，kubernetes通过Namespace和Context设置对不同工作组进行区分

a、创建命名空间

    # namespace-development.yaml
    apiVersion: v1
    kind: Namespace
    metadata:
      name: development

    # namespace-production.yaml
    apiVersion: v1
    kind: Namespace
    metadata:
      name: production

b、定义context

分别对工作组定义一个context，即运行环境。这个运行环境将属于某个特定的命名空间

```
kubectl config set-cluster kubernetes-cluster --server=https://192.168.230.128:8080
kubectl config set-context ctx-dev --namespace=development --cluster=kubernetes-cluster --user=dev
kubectl config set-context ctx-prod --namespace=production --cluster=kubernetes-cluster --user=prod
```

查看已定义的context,文件位置在${HOME}/.kube/目录下

```
kubectl config view 
```

c、设置工作组在特定context中工作
设置当前运行环境，所有操作在对应namespace下完成

```
kubectl config use-context <context_name>
```



4、k8s资源管理

计算资源管理（compute resources）、服务质量管理(Qos、优先级)、资源配额管理(LimitRange、ResourceQuota)

查看事件

```
kubectl describe pod <pod_name> | grep -A 3 Events
```

查看可用资源容量上限（Capacity）和已分配资源量（Allocated resources）

```
kubectl describe nodes <node_name>
```



5、日志集群搭建

查看K8S基础组件日志

```
journalctl -b -u xxx.service

kubectl logs <pod_name> -n kube-system
```



### Fluentd Elasticsearch+ Kibana 部署

1、Node节点部署flunted容器采集系统组件日志/var/log和容器日志/var/lib/docker/containers
2、flunted采集汇总到Elasticsearch保存
3、用户通过kibana提供的Web页面查询

```
kubectl apply -f elasticsearch.yaml
//首先需要在对应node上创建hostpath目录
```

    kubectl get svc -n kube-system
    NAMETYPECLUSTER-IP  EXTERNAL-IP   PORT(S)AGE
    elasticsearch   ClusterIP   10.97.238.132   <none>9200/TCP   40m
    
    curl 10.97.238.132:9200
    
    {
      "name" : "elasticsearch-7d4fdc8b75-plx7c",
      "cluster_name" : "elasticsearch",
      "cluster_uuid" : "fX2laIx8TZi3JFg_EObGbA",
      "version" : {
         "number" : "7.5.1",
    	 "build_flavor" : "default",
         "build_type" : "docker",
         "build_hash" :  "3ae9ac9a93c95bd0cdc054951cf95d88e1e18d96",
         "build_date" : "2019-12-16T22:57:37.835892Z",
         "build_snapshot" : false,
         "lucene_version" : "8.3.0",
         "minimum_wire_compatibility_version" : "6.8.0",
         "minimum_index_compatibility_version" : "6.0.0-beta1"
      },
      "tagline" : "You Know, for Search"
    }

```
kubectl apply -f flunted.yaml 

curl "10.97.238.132:9200/_cat/indices?v"  //查看创建的索引信息

kubectl apply -f kibana.yaml 
```

    kubectl cluster-info    //kibana服务访问url地址
    
    [root@k8s-master es]# kubectl cluster-info
    Kubernetes control plane is running at https://192.168.230.128:6443
    Elasticsearch is running at https://192.168.230.128:6443/api/v1/namespaces/kube-system/services/elasticsearch/proxy
    Kibana is running at https://192.168.230.128:6443/api/v1/namespaces/kube-system/services/kibana/proxy
    CoreDNS is running at https://192.168.230.128:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
    KubeDNSUpstream is running at https://192.168.230.128:6443/api/v1/namespaces/kube-system/services/kube-dns-upstream:dns/proxy



###helm使用

    helm repo add stable https://charts.helm.sh/stable //添加官方稳定版chart仓库
    helm search repo stable   //查询对应仓库可部署的chart列表
    helm search hub        //搜索由Artifact Hub提供的来自不同仓库的大量chart列表
    
    helm repo list   //查看已经添加的Chart仓库列表
    helm repo add <repo_name> <repo_url>  //添加新的仓库
    helm repo update   //确保本地仓库数据更新
    helm repo remove <repo_name>   //从本地删除仓库
    
    helm install <release_name> <chart_name>    //部署chart应用，release实例，chart_name可以是压缩包，解压缩的Chart目录，一个完整的URL
    helm list   //查看已部署的release列表
    helm status   //查看部署状态
    
    在部署之前自定义Chart的配置数据，修改相关默认配置自定义安装
    helm show values <chart_name>    //查看chart的可配置项
    
    用户编写YAML文件覆盖默认内容，在安装时引用配置文件
    eg:
    echo ' {mariadbUser: userO , mariadbDatabase : userOdb} ' > config.yaml
    helm install -f config.yaml mariadb-1 stable/mariadb
    helm install --set name=value,name1=value1 mariadb-1 stable/mariadb
    
    helm upgrade -f xxx.yaml <release_name> <chart_name>
    helm get values <release_name>    //查看配置内容
    
    helm rollback <release_name> <reversion_num>  //回滚到某个版本
    helm history <release_name>   //查看release修订历史记录
    
    helm uninstall <release_name>    //卸载一个Release
    helm uninstall <release_name> --keep-history    //保留删除记录
    helm list --uninstalled     //查看保留的卸载记录
    
    helm create <chart_name>    //创建一个Chart模板
    helm lint <chart_name>     //验证Chart的各文件格式是否正确
    helm package <chart_name>   //将chart打包.tgz文件
    helm install <release_name> xxx.tgz
    
    helm repo index /xx/xx/xx --url http://xx.xx.xx.xx/charts   //根据目录下的chart内容创建index.yaml索引文件
    helm repo add <repo_name> <url>   //添加仓库到Helm客户端



###trouble shooting 相关命令

    kubectl describe pod <pod_name> | grep Events  //查看pod事件
    node、svc、rc、namespace、secrets、deployments、ds
    
    kubectl logs <pod_name> -c <container_name>  //查看容器日志
    
    systemctl status kube-apiserver
    journalctl -u kube-apiserver