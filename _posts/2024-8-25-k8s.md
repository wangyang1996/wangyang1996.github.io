---

layout:     post                    # 使用的布局（不需要改）
title:      k8s 基础学习            # 标题 
subtitle:   k8s                     #副标题
date:       2024-08-25             # 时间
author:     wangyang                     # 作者
header-img: img/post-bg-desk.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - k8s
---


k8s学习
=================
本篇文章记录k8s基础内容学习，命令和组件等

1.k8s命令
--------------------------

```shell
kubectl get nodes         //查看集群nodes信息
kubectl cluster-info      //查看集群信息
kubectl get pods -o wide  //查看pods信息
kubectl get deployments   //查看deployment应用信息
kubectl get rs            //查看rs信息
kubeadm token list        //查看token
kubectl get service       //查看端口映射信息
kubectl delete 资源类型 name  //删除某类资源类型
```

## 2.Controller控制器

### 2.1.Deployment

```shell
kubectl expose pod pod_name --type="NodePort" --port container_port     
//将容器container内的端口映射到当前节点node上的端口，方便外部访问容器服务
kubectl expose deployment deployment_name --type="ClusterIP" --port container_port     
//将容器container内的端口映射到集群IP上端口，方便经统一IP访问容器服务，实现负载均衡

docker exec -it container_name bash    //从对应node节点进入container容器

kubectl scale deployment deployment_name --replicas=num   //实现pod伸缩容 

kubectl set image deployment deployment_name app_name(deployment的app，即container)=image版本  
//滚动更新,更新过程中，pod逐个更新，逐个删除旧pod，rs资源会切换到新资源上
kubectl rollout undo deployment deployment_name   
//回滚版本,回滚过程中，pod逐个创建新pod,rs资源会切换到旧资源上

kubectl get pods --all-namespace  -o wide  //查看全部namespace下的pod
kubectl describe pod pod_name       //查看pod具体情况
kubectl describe 资源 资源名称。      //查看详细信息
systemctl status kubelet.service    //kubelet是唯一没有以容器形式运行的k8s组件
kubectl run deployment_name --image=xxx  --replicas=num   
//kubectl运行pod，创建过程：kubectl->apiserver->conroller manager->scheduler->kubelet

kubectl taint node k8s-master node-role.kubernetes.io/master-
//将master节点当作Node使用
kubectl taint node k8s-master node-role.kubernetes.io/master="":NoSchedule
//master节点不当Node使用

kubectl label node node_name key=value  
//为节点添加标签，可以在yml的pod规格spec.nodeSelector设置key: value，那么所有pod会部署到对应标签节点
kubectl get node --show-labels  //查看节点label
kubectl label node node_name key-
//删除标签，-即删除，删除需重新部署才会生效

```

nginx_pod_deployment.yml 

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
        app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```



### 2.2.DaemonSet

```
每个node上最多只能运行一个副本
场景：集群每个节点存储Daemon、日志收集Daemon、监控Daemon
```

```shell
kubectl get daemonset -n kube-system     //会显示默认的kube-proxy
kubectl edit daemonset kube-proxy -n kube-system  //查看配置和运行状态,status
kubectl edit 资源类型 资源名称              //查看资源配置和运行状态，status

kubectl create namespace 名称             //创建命名空间
kubectl get namespace
```

node_exporter.yml

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter-daemonset
  namespace: monitor
spec:
  selector:
    matchLabels:
        app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      hostNetwork: true
      containers:
      - name: node-exporter
        image: prom/node-exporter
        command:
        - /bin/node_exporter
        - --path.procfs
        - /host/proc
        - --path.sysfs
        - /host/sys
        - --collector.filesystem.ignored-mount-points
        - ^/(sys|proc|dev|host|etc)($|/)
        volumeMounts:
        - name: proc
          mountPath: /host/proc
        - name: sys
          mountPath: /host/sys
        - name: root
          mountPath: /rootfs
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      - name: root
        hostPath:
          path: /
```



### 2.3.Job

```
容器按持续运行时间可分为两类：服务类容器和工作类容器
服务类容器持续提供服务
工作类容器则是一次性任务，比如批处理程序，完成后容器就退出
```

myjob.yml

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: myjob-job
spec:
  completions: 6              //执行总数
  parallelism: 2              //每次并行pod数量
  template:
    metadata:
      labels:
        app: myjob
    spec:
      containers:
      - name: hello
        image: busybox
        command: ["echo", "k8s job"]
      restartPolicy: Never
```

```shell
kubectl get job                //查看job执行情况
kubectl get pod                //查看job执行pod状态
kubectl logs pod_name          //查看job资源执行情况
```

```
Job资源pod如果执行失败，SUCCESSFUL的pod数量不为1的话，
根据配置的restartPolicy重启策略pod会重启直至成功pod为1；
restartPolicy: Never           //会出现多个执行失败pod
restartPolicy: OnFailure       //同一个pod一直重启，RESTARTS数量一直增加直到成功
```

### 2.4.CronJob

前置条件： Kube-apiserver支持batch/v1

```shell
cat /etc/kubernetes/manifests/kube-apiserver.yaml
systemctl restart kubelet.service        //如果修改重启生效
kubectl api-versions                     //确认是否支持
```

crontab.yml   

```yaml
kind: CronJob
metadata:
  name: cronjob
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            command: ["echo", "k8s job"]
          restartPolicy: OnFailure
```

```shell
kubectl get cronjob            //查看定时任务
kubectl get job                //查看每次任务执行情况
kubectl get pod                //查看每次任务执行pod
kubectl logs pod_name          //查看每次pod执行日志
kubectl delete -f cronjob.yml  //删除crontab资源
```

## 3.service

### 3.1.创建service

给上文的nginx_pod_deployment.yml创建的nginx pod做service

nginx_service.yml

```yaml
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 80
```

```shell
kubectl get service
kubectl describe service service_name     //查看service和pod的对应关系
//Endpoints:         10.244.1.22:80,10.244.2.20:80,10.244.2.21:80
```

Cluster IP是一个虚拟IP，由k8s节点上iptables规则管理

```shell
iptables-save                            //打印出当前节点的iptables规则

[root@k8s-master k8s-yaml]# iptables-save | grep 10.100.57.112
-A KUBE-SERVICES -d 10.100.57.112/32 -p tcp -m comment --comment "default/nginx-service cluster IP" -m tcp --dport 8080 -j KUBE-SVC-V2OKYYMBY3REGZOG
//其他源地址访问nginx-service,跳转到规则KUBE-SVC-V2OKYYMBY3REGZOG
-A KUBE-SVC-V2OKYYMBY3REGZOG ! -s 10.244.0.0/16 -d 10.100.57.112/32 -p tcp -m comment --comment "default/nginx-service cluster IP" -m tcp --dport 8080 -j KUBE-MARK-MASQ
//如果Cluster内的pod要访问nginx-service,则允许


[root@k8s-master k8s-yaml]# iptables-save | grep KUBE-SVC-V2OKYYMBY3REGZOG
-A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment "default/nginx-service -> 10.244.1.22:80" -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-LSK55P2YY4VA4YTL
// 1/3的概率跳转到规则KUBE-SEP-LSK55P2YY4VA4YTL
-A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment "default/nginx-service -> 10.244.2.20:80" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-ONDQTO2GE7J66ITK
//剩下2/3的1/2，即1/3跳转到规则KUBE-SEP-ONDQTO2GE7J66ITK
-A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment "default/nginx-service -> 10.244.2.21:80" -j KUBE-SEP-SLTKXEH2UHIG4YH2
//剩下1/3跳转到规则KUBE-SEP-SLTKXEH2UHIG4YH2

[root@k8s-master k8s-yaml]# iptables-save | grep 具体规则
-A KUBE-SEP-LSK55P2YY4VA4YTL -s 10.244.1.22/32 -m comment --comment "default/nginx-service" -j KUBE-MARK-MASQ        
//从pod容器内部访问cluster IP
-A KUBE-SEP-LSK55P2YY4VA4YTL -p tcp -m comment --comment "default/nginx-service" -m tcp -j DNAT --to-destination 10.244.1.22:80

-A KUBE-SEP-ONDQTO2GE7J66ITK -s 10.244.2.20/32 -m comment --comment "default/nginx-service" -j KUBE-MARK-MASQ
-A KUBE-SEP-ONDQTO2GE7J66ITK -p tcp -m comment --comment "default/nginx-service" -m tcp -j DNAT --to-destination 10.244.2.20:80

-A KUBE-SEP-SLTKXEH2UHIG4YH2 -s 10.244.2.21/32 -m comment --comment "default/nginx-service" -j KUBE-MARK-MASQ
-A KUBE-SEP-SLTKXEH2UHIG4YH2 -p tcp -m comment --comment "default/nginx-service" -m tcp -j DNAT --to-destination 10.244.2.21:80


上述规则表明访问service的请求分别转发到后端的三个pod，负载均衡策略
Cluster每一个节点都配置相同的iptables规则，确保整个集群都可以通过Cluster IP访问到Service
```

### 3.2.DNS访问Service

每当新建Service，coreDns会添加该Service的DNS记录，Cluster中的Pod可以通过<SERVICE_NAME>.<NAMESPACE_NAME>访问Service

```shell
[root@k8s-master k8s-yaml]# kubectl get deployment -n kube-system
NAME      READY   UP-TO-DATE   AVAILABLE   AGE
coredns   2/2     2            2           40d
[root@k8s-master k8s-yaml]# kubectl get pod -n kube-system | grep dns
coredns-66f779496c-2l6zq             1/1     Running   18 (19h ago)   40d
coredns-66f779496c-rjq6v             1/1     Running   18 (19h ago)   40d
```

```shell
[root@k8s-master k8s-yaml]# kubectl exec -it  nginx-deployment-7c5ddbdf54-9pf5l bash
root@nginx-deployment-7c5ddbdf54-9pf5l:/# curl nginx-service.default:8080
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

Pod和Service同属一个namespace的话，可以省略namespace_name，通过<SERVICE_NAME>访问Service

```shell
root@nginx-deployment-7c5ddbdf54-9pf5l:/# curl nginx-service:8080
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

```shell
root@nginx-deployment-7c5ddbdf54-9pf5l:/# nslookup nginx-service
;; Got recursion not available from 10.96.0.10
Server:         10.96.0.10
Address:        10.96.0.10#53

Name:   nginx-service.default.svc.cluster.local
Address: 10.100.57.112
;; Got recursion not available from 10.96.0.10
```

nginx-service的完整域名为nginx-service.default.svc.cluster.local

多个资源可以在一个YAML文件中定义，需要用`---`分割

Pod和Service不属一个namespace的话，必须通过<SERVICE_NAME>.<NAMESPACE_NAME>访问Service



### 3.3.外网访问Service

```
Service类型：

ClusterIP:
Service通过Cluster内部的IP对外提供服务，只有Cluster内的节点和Pod可以访问

NodePort:
Service通过Cluster节点的静态端口对外提供服务。Cluster外部可以通过<NodeIP>:<NodePort>访问Service
```

nginx_service_nodeport.yml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service-nodeport
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
  - protocol: TCP
    nodePort: 32000            //node节点上监听的端口
    port: 8090                 //ClusterIP上监听的端口
    targetPort: 80             //Pod上监听的端口
```

相当于在ClusterIP 的Service类型上添加了node节点端口的映射

```shell
[root@k8s-master k8s-yaml]# kubectl get service
NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes               ClusterIP   10.96.0.1       <none>        443/TCP          40d
nginx-service            ClusterIP   10.100.57.112   <none>        8080/TCP         6h18m
nginx-service-nodeport   NodePort    10.103.150.34   <none>        8090:32000/TCP   7s

[root@k8s-master k8s-yaml]# curl 10.103.150.34:8090
<html>
<title>Welcome to nginx!</title>
</html>

[root@k8s-master k8s-yaml]# kubectl get nodes -o wide
NAME         STATUS   ROLES           AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION           CONTAINER-RUNTIME
k8s-master   Ready    control-plane   40d   v1.28.2   192.168.230.128   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://26.1.4
k8s-node1    Ready    <none>          40d   v1.28.2   192.168.230.129   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://26.1.4
k8s-node2    Ready    <none>          40d   v1.28.2   192.168.230.130   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://26.1.4
[root@k8s-master k8s-yaml]# curl 192.168.230.129:32000
<html>
<title>Welcome to nginx!</title>
</html>

//外部可通过<NodeIP>:<NodePort>访问Service，浏览器验证
```

```shell
[root@k8s-master k8s-yaml]# iptables-save | grep SW7F3NCRFKRM23GY
:KUBE-EXT-SW7F3NCRFKRM23GY - [0:0]
:KUBE-SVC-SW7F3NCRFKRM23GY - [0:0]

-A KUBE-EXT-SW7F3NCRFKRM23GY -m comment --comment "masquerade traffic for default/nginx-service-nodeport external destinations" -j KUBE-MARK-MASQ
-A KUBE-EXT-SW7F3NCRFKRM23GY -j KUBE-SVC-SW7F3NCRFKRM23GY
-A KUBE-NODEPORTS -p tcp -m comment --comment "default/nginx-service-nodeport" -m tcp --dport 32000 -j KUBE-EXT-SW7F3NCRFKRM23GY
//NodePort相关iptables规则，请求节点端口32000时,会应用请求到pod端口的KUBE-SVC-SW7F3NCRFKRM23GY规则

-A KUBE-SERVICES -d 10.103.150.34/32 -p tcp -m comment --comment "default/nginx-service-nodeport cluster IP" -m tcp --dport 8090 -j KUBE-SVC-SW7F3NCRFKRM23GY
-A KUBE-SVC-SW7F3NCRFKRM23GY ! -s 10.244.0.0/16 -d 10.103.150.34/32 -p tcp -m comment --comment "default/nginx-service-nodeport cluster IP" -m tcp --dport 8090 -j KUBE-MARK-MASQ
//ClusterIP相关iptables规则，请求ClusterIP端口8090时,会应用请求到pod端口的KUBE-SVC-SW7F3NCRFKRM23GY规则

-A KUBE-SVC-SW7F3NCRFKRM23GY -m comment --comment "default/nginx-service-nodeport -> 
10.244.1.22:80" -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-Z5SU7NYCFI2K7HIR
-A KUBE-SVC-SW7F3NCRFKRM23GY -m comment --comment "default/nginx-service-nodeport -> 10.244.2.20:80" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-AKGUI2FJZHFCP6QL
-A KUBE-SVC-SW7F3NCRFKRM23GY -m comment --comment "default/nginx-service-nodeport -> 10.244.2.21:80" -j KUBE-SEP-MM6NPVJ7O6YETGXF
//Pod相关规则
```

NodePort和ClusterIP在各自端口接收到的请求都会通过iptables转发到Pod的targetPort



## 4.Rolling Update

滚动更新的最大好处是零停机，整个更新过程始终有副本在运行，从而保证业务的连续性。

```shell
kubectl apply -f deployment.yml
kubectl get deployment -o wide
kubectl get replicaset -o wide
kubectl get pod -o wide
```

更新deploymnet.yml 镜像版本，重新apply

```shell
kubectl apply -f deployment.yml
kubectl get deployment -o wide            //镜像版本变化
kubectl get replicaset -o wide            //多条replicaset资源
kubectl get pod -o wide                   //逐个更新pod，创建一个新rs的pod，删除一个老rs的pod
```



回滚

kubectl apply 每次更新应用时，k8s都会记录下当前的配置，保存一个版次 revision;

可以通过deployment.yml 文件中spec.revisionHistoryLimit保留最近n次；

```shell
kubectl apply -f deployment.yml --record        
//--record将当前命令记录到revision记录中的CHANGE-CAUSE列信息，方便回滚知道具体文件、版本

kubectl rollout history deployment deployment_name  
//查看revision历史记录,查看REVISION列版本标识数字,可通过CHANGE-CAUSE列记录导入时的文件、命令，
//查看对应yaml文件内容，所以每次更新最好新建yaml文件对同一deployment资源进行更新

kubectl rollout undo deployment deployment_name --to-revision=num    //回滚到具体版本
```



## 5.Health Check

默认健康检查 ： yaml文件定义的spec.restartPolicy重启策略：always,OnFailure,

Liveness和Readiness探测机制提供更精细的健康检查

```
Liveness探测让用户可以自定义判断容器是否健康的条件
spec.containers.livenessProbe

spec.containers.livenessProbe.initialDelaySeconds  容器启动多少秒后开始执行Liveness探测
spec.containers.livenessProbe.periodSeconds  容器每多少秒执行一次Liveness探测，连续3次执行失败，会杀掉并重启容器
```

```
Readiness探测告诉k8s什么时候可以将容器加入Service负载均衡池中，对外提供服务
spec.containers.readinessProbe

spec.containers.readinessProbe.initialDelaySeconds  容器启动多少秒后开始执行Readiness探测
spec.containers.readinessProbe.periodSeconds  容器每多少秒执行一次Readiness探测，连续3次执行失败，会将容器状态READY设置为不可用
```

```
区别：
用Liveness探测容器是否需要重启以实现自愈
用Readiness探测判断容器是否已经准备好对外提供服务

面对探测失败的处理方式：
Liveness探测是重启容器
Readiness探测是将容器设置为不可用，不接收Service转发的请求
```

```yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  template:
    metadata:
      labels:
        run: web
    spec:
      containers:
      - name: web
        image: myhttpd
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:                  //httpGet探测方法，http请求的返回值在200-400之间则为探测成功
            scheme: HTTP
            path: /healthy
            port: 8080             //探测http://[container_ip]:8080/healthy,探测成功才加入web-svc负载均衡中
          initialDelaySeconds: 10
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: web-svc
spec:
  selector:
    run: web
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 80

```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  replicas: 4
  selector:                     //要选择selector匹配标签
    matchLabels:
      run: app
  template:
    metadata:
      labels:
        run: app
    spec:
      containers:
      - name: app
        image: busybox
        args:
        - /bin/sh
        - -c
        - sleep 10; touch /tmp/healthy; sleep 30000
        readinessProbe:
          exec:
            command:
            - cat
            - /tmp/healthy
          initialDelaySeconds: 10
          periodSeconds: 5
```

```shell
kubectl get deployment 资源  相关参数说明：
DESIRED：期望副本数量
CURRENT：当前副本数量，新副本+旧副本
UP-TO-DATE：完成更新副本数量，即几个新副本
AVAILABLE：当前处于READY状态的副本数量，即几个旧副本

kubectl describe deployment 资源 
滚动更新时显示更新状态
更新时默认参数配置
RollingUpdateStrategy:  25% max unavailable, 25% max surge
max surge 控制滚动更新时副本总数超过DESIRED的上限，向上取整
max unavailable 控制滚动更新时不可用的副本占DESIRED的上限，向下取整

示例：
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  13m   deployment-controller  Scaled up replica set app-757c489674 to 4
  Normal  ScalingReplicaSet  106s  deployment-controller  Scaled up replica set app-5bd8fc758c to 1         //新副本增加1个，max surge，总数4+4*25% = 5个
  Normal  ScalingReplicaSet  106s  deployment-controller  Scaled down replica set app-757c489674 to 3 from 4  //旧副本减少1个，max unavailable ，总数4个
  Normal  ScalingReplicaSet  106s  deployment-controller  Scaled up replica set app-5bd8fc758c to 2 from 1  //旧副本减少1个，新副本新建1个，使总数保持在 4+4*25% = 5个

新副本因为没有通过Readiness检测，后续更新卡住，整体情况为：
Replicas:               4 desired | 2 updated | 5 total | 3 available | 2 unavailable
                        期待4个      更新2个      总数5个    3个可用，旧副本  2个不可用，新副本


[root@k8s-master k8s-yaml]# kubectl rollout history deployment app
deployment.apps/app 
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=app.v1.yml --record=true
2         kubectl apply --filename=app.v2.yml --record=true

[root@k8s-master k8s-yaml]# kubectl rollout undo deployment app --to-revision=1
deployment.apps/app rolled back
```

## 6.数据管理

Volume的生命周期独立于容器



1、emptyDir

最基础的volume类型，emptyDir对于容器来说是持久的，对于Pod则不是。Pod从节点删除时，volume的内容也被删除。emptyDir的生命周期和Pod一致

Pod中的所有容器都可以共享volume，可以指定各自的mount路径

emptyDir.yml：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: producer-consumer
spec:   
  containers:
  - name: producer
    image: busybox
    volumeMounts:
    - mountPath: /producer_dir
      name: shared-volume
    args:
    - /bin/sh
    - -c
    - echo "hello world" > /producer_dir/hello ; sleep 30000

  - name: consumer
    image: busybox
    volumeMounts:
    - mountPath: /consumer_dir
      name: shared-volume
    args:
    - /bin/sh
    - -c
    - cat /consumer_dir/hello ; sleep 30000

  volumes:
  - name: shared-volume
    emptyDir: {}
```

```shell
[root@k8s-master k8s-yaml]# kubectl apply -f emptyDir.yml
[root@k8s-master k8s-yaml]# kubectl logs producer-consumer consumer
hello world
相当于在对应node节点执行docker run -v /producer_dir 和 docker run -v /consumer_dir，临时目录。
在对应node节点执行 “docker inspect 容器名” ，可以发现volume位置相同

 "Mounts": [
            {
                "Type": "bind",
                "Source": "/var/lib/kubelet/pods/d3c43aab-3943-41bd-9f57-2a695b5ea935/volumes/kubernetes.io~empty-dir/shared-volume",
                "Destination": "/consumer_dir",
                "Mode": "",
                "RW": true,
                "Propagation": "rprivate"
            }
            ]
pod不存在，emptyDir也对应消失
```



2、hostPath

将Docker Host文件系统中已经存在的目录mount给Pod的容器；

Pod被销毁，hostPath对应的目录还会被保留，持久性比emptyDir强，但是也增加了pod和node节点的耦合性；

Host主机即node节点崩溃，Pod迁移到其他节点，hostPath则无法访问

`kubectl edit --namespace=kube-system pod kube-apiserver-k8s-master`

```yaml
 volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/pki
      type: DirectoryOrCreate
    name: etc-pki
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
```



3、外部Storage Provider

比如云厂商提供的云硬盘或者分布式存储，不同配置参考云厂商官方文档；

存储和k8s集群分离，保证了数据的持久化；



4、PresistentVolume & PresistentVolumeClaim

PV是外部存储系统中的一块存储空间，PV具有持久性，生命周期独立于Pod;

PVC是对PV的申请，指明存储容量大小和访问模式等信息，k8s会查找并提供满足条件的PV；

例子：

先master节点安装NFS

```shell
安装nfs和rpc
yum install -y nfs-utils rpcbind 
启动并设置自启动
systemctl start rpcbind
systemctl start nfs-server
systemctl enable rpcbind
systemctl enable nfs-server
查看状态
systemctl status nfs
systemctl status rpcbind
创建nfs目录
mkdir -p /nfsdata
nfs服务端访问权限
vi /etc/exports
/nfsdata *(rw,sync,no_root_squash)
重启nfs和rpc
systemctl restart nfs rpcbind
验证
showmount -e 127.0.0.1
Export list for 127.0.0.1:
/nfsdata *
```

创建PV

nfs-pv1.yml

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mypv1
spec:
  capacity:
    storage: 1Gi           //PV容量
  accessModes:             //共三种模式，ReadWriteOnce 以读写模式mount到单个节点
    - ReadWriteOnce        //ReadOnlyMany 以只读模式mount到多个节点
                           //ReadWriteMany 以读写模式mount到多个节点
  persistentVolumeReclaimPolicy: Recycle      //PV回收策略
                                      //Retain保留PV数据，管理员决定是否手工回收
                                      //Recycle清除PV中的数据，相关于rm -rf /thevolume/*
                                      //Delete删除PV在存储资源对应的存储空间
  storageClassName: nfs      //指定PV的class为nfs,PVC可以指定class申请对应class的PV
  nfs:
    path: /nfsdata/pv1       //指定PV在NFS服务器上对应的目录，需对应创建/nfsdata/pv1目录
    server: 192.168.230.128
```

```shell
[root@k8s-master k8s-yaml]# kubectl get pv
NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
mypv1   1Gi        RWO            Recycle          Available           nfs                     6s
```

创建PVC

nfs-pvc1.yml

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc1
spec:
  accessModes:
    - ReadWriteOnce            //指定访问模式
  resources:
    requests:
      storage: 1Gi             //指定容量
  storageClassName: nfs        //指定class
```

```shell
[root@k8s-master k8s-yaml]# kubectl get pvc
NAME     STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mypvc1   Bound    mypv1    1Gi        RWO            nfs            6s
[root@k8s-master k8s-yaml]# kubectl get pv
NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGE
mypv1   1Gi        RWO            Recycle          Bound    default/mypvc1   nfs                     12m
Bound    default/mypvc1 ：PVC已经Bound到PV
```

现在在Pod中使用存储，Pod使用PVC访问PV资源

Pod-nfs.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod1
spec:
  containers:
  - name: mypod1
    image: busybox
    args:
    - /bin/sh
    - -c
    - sleep 30000
    volumeMounts:
    - mountPath: "/mydata"       //vloume mydata 挂载到容器/mydata目录
      name: mydata
  volumes:
    - name: mydata
      persistentVolumeClaim:
        claimName: mypvc1
```

```
kubectl exec  mypod1  touch /mydata/hello
ll /nfsdata/pv1/
hello文件被创建
```

回收PV

不需要使用PV时，可以通过删除PVC回收PV

回收策略为Recycle，PV状态由Released(解除)，新起Pod清理完PV的数据后，PV状态变为Available(可用)

```shell
[root@k8s-master k8s-yaml]# kubectl delete pvc mypvc1
persistentvolumeclaim "mypvc1" deleted
[root@k8s-master k8s-yaml]# kubectl get pod -o wide
NAME                                READY   STATUS    RESTARTS         AGE     IP            NODE        NOMINATED NODE   READINESS GATES
mypod1                              1/1     Running   0                22m     10.244.1.46        k8s-node1   <none>           <none>
recycler-for-mypv1                  0/1     ContainerCreating   0                3s      <none>        k8s-node1   <none>           <none>
//启动一个新Pod recycler-for-mypv1 清除PV 数据
[root@k8s-master k8s-yaml]# kubectl get pv
NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM            STORAGECLASS   REASON   AGE
mypv1   1Gi        RWO            Recycle          Released   default/mypvc1   nfs                     49m
//mypv1状态为Released，解除与mypvc1的Bound，正在清除数据，此时还不可用
[root@k8s-master k8s-yaml]# kubectl get pv
NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM            STORAGECLASS   REASON   AGE
mypv1   1Gi        RWO            Recycle          Available                   nfs                     49m
//PV数据被清除后，mypv1状态变为Available
```

回收策略为Retain，删除PVC后，PV状态一直都是Released(解除)，不能被其他PVC申请。需要删除并重新创建PV，PV状态变为Available(可用)，数据同时被保留。

上述创建PV的方式为静态供给；

PV动态创建，即没有满足PVC条件的PV，会动态创建PV。`kind : StorageClass` 

StorageClass定义如何创建PV

StorageClass支持Delete和Retain两种回收策略，默认为Delete

PVC申请PV时，只需要指定StorageClass、容量以及访问模式即可