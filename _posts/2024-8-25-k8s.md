---

layout:     post                    # 使用的布局（不需要改）
title:      k8s 基础学习            # 标题 
subtitle:   k8s                     #副标题
date:       2024-08-25             # 时间
author:     wangyang                     # 作者
header-img: img/post-bg-desk.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - k8s
---


k8s学习
=================
本篇文章记录k8s基础内容学习，命令和组件等

1.k8s命令
--------------------------

```shell
kubectl get nodes         //查看集群nodes信息
kubectl cluster-info      //查看集群信息
kubectl get pods -o wide  //查看pods信息
kubectl get deployments   //查看deployment应用信息
kubectl get rs            //查看rs信息
kubeadm token list        //查看token
kubectl get service       //查看端口映射信息
kubectl delete 资源类型 name  //删除某类资源类型
kubectl delete 资源类型 name  --grace-period=0 --force //强制删除某类资源
```

## 2.Controller控制器

### 2.1.Deployment

```shell
kubectl expose pod pod_name --type="NodePort" --port container_port     
//将容器container内的端口映射到当前节点node上的端口，方便外部访问容器服务
kubectl expose deployment deployment_name --type="ClusterIP" --port container_port     
//将容器container内的端口映射到集群IP上端口，方便经统一IP访问容器服务，实现负载均衡

docker exec -it container_name bash    //从对应node节点进入container容器

kubectl scale deployment deployment_name --replicas=num   //实现pod伸缩容 

kubectl set image deployment deployment_name app_name(deployment的app，即container)=image版本  
//滚动更新,更新过程中，pod逐个更新，逐个删除旧pod，rs资源会切换到新资源上
kubectl rollout undo deployment deployment_name   
//回滚版本,回滚过程中，pod逐个创建新pod,rs资源会切换到旧资源上

kubectl get pods --all-namespace  -o wide  //查看全部namespace下的pod
kubectl describe pod pod_name       //查看pod具体情况
kubectl describe 资源 资源名称。      //查看详细信息
systemctl status kubelet.service    //kubelet是唯一没有以容器形式运行的k8s组件
kubectl run deployment_name --image=xxx  --replicas=num   
//kubectl运行pod，创建过程：kubectl->apiserver->conroller manager->scheduler->kubelet

kubectl taint node k8s-master node-role.kubernetes.io/master-
//将master节点当作Node使用
kubectl taint node k8s-master node-role.kubernetes.io/master="":NoSchedule
//master节点不当Node使用

kubectl label node node_name key=value  
//为节点添加标签，可以在yml的pod规格spec.nodeSelector设置key: value，那么所有pod会部署到对应标签节点
kubectl get node --show-labels  //查看节点label
kubectl label node node_name key-
//删除标签，-即删除，删除需重新部署才会生效

```

nginx_pod_deployment.yml 

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
        app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```



### 2.2.DaemonSet

```
每个node上最多只能运行一个副本
场景：集群每个节点存储Daemon、日志收集Daemon、监控Daemon
```

```shell
kubectl get daemonset -n kube-system     //会显示默认的kube-proxy
kubectl edit daemonset kube-proxy -n kube-system  //查看配置和运行状态,status
kubectl edit 资源类型 资源名称              //查看资源配置和运行状态，status

kubectl create namespace 名称             //创建命名空间
kubectl get namespace
```

node_exporter.yml

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter-daemonset
  namespace: monitor
spec:
  selector:
    matchLabels:
        app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      hostNetwork: true
      containers:
      - name: node-exporter
        image: prom/node-exporter
        command:
        - /bin/node_exporter
        - --path.procfs
        - /host/proc
        - --path.sysfs
        - /host/sys
        - --collector.filesystem.ignored-mount-points
        - ^/(sys|proc|dev|host|etc)($|/)
        volumeMounts:
        - name: proc
          mountPath: /host/proc
        - name: sys
          mountPath: /host/sys
        - name: root
          mountPath: /rootfs
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      - name: root
        hostPath:
          path: /
```



### 2.3.Job

```
容器按持续运行时间可分为两类：服务类容器和工作类容器
服务类容器持续提供服务
工作类容器则是一次性任务，比如批处理程序，完成后容器就退出
```

myjob.yml

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: myjob-job
spec:
  completions: 6              //执行总数
  parallelism: 2              //每次并行pod数量
  template:
    metadata:
      labels:
        app: myjob
    spec:
      containers:
      - name: hello
        image: busybox
        command: ["echo", "k8s job"]
      restartPolicy: Never
```

```shell
kubectl get job                //查看job执行情况
kubectl get pod                //查看job执行pod状态
kubectl logs pod_name          //查看job资源执行情况
```

```
Job资源pod如果执行失败，SUCCESSFUL的pod数量不为1的话，
根据配置的restartPolicy重启策略pod会重启直至成功pod为1；
restartPolicy: Never           //会出现多个执行失败pod
restartPolicy: OnFailure       //同一个pod一直重启，RESTARTS数量一直增加直到成功
```

### 2.4.CronJob

前置条件： Kube-apiserver支持batch/v1

```shell
cat /etc/kubernetes/manifests/kube-apiserver.yaml
systemctl restart kubelet.service        //如果修改重启生效
kubectl api-versions                     //确认是否支持
```

crontab.yml   

```yaml
kind: CronJob
metadata:
  name: cronjob
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            command: ["echo", "k8s job"]
          restartPolicy: OnFailure
```

```shell
kubectl get cronjob            //查看定时任务
kubectl get job                //查看每次任务执行情况
kubectl get pod                //查看每次任务执行pod
kubectl logs pod_name          //查看每次pod执行日志
kubectl delete -f cronjob.yml  //删除crontab资源
```

## 3.service

### 3.1.创建service

给上文的nginx_pod_deployment.yml创建的nginx pod做service

nginx_service.yml

```yaml
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 80
```

```shell
kubectl get service
kubectl describe service service_name     //查看service和pod的对应关系
//Endpoints:         10.244.1.22:80,10.244.2.20:80,10.244.2.21:80
```

Cluster IP是一个虚拟IP，由k8s节点上iptables规则管理

```shell
iptables-save                            //打印出当前节点的iptables规则

[root@k8s-master k8s-yaml]# iptables-save | grep 10.100.57.112
-A KUBE-SERVICES -d 10.100.57.112/32 -p tcp -m comment --comment "default/nginx-service cluster IP" -m tcp --dport 8080 -j KUBE-SVC-V2OKYYMBY3REGZOG
//其他源地址访问nginx-service,跳转到规则KUBE-SVC-V2OKYYMBY3REGZOG
-A KUBE-SVC-V2OKYYMBY3REGZOG ! -s 10.244.0.0/16 -d 10.100.57.112/32 -p tcp -m comment --comment "default/nginx-service cluster IP" -m tcp --dport 8080 -j KUBE-MARK-MASQ
//如果Cluster内的pod要访问nginx-service,则允许


[root@k8s-master k8s-yaml]# iptables-save | grep KUBE-SVC-V2OKYYMBY3REGZOG
-A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment "default/nginx-service -> 10.244.1.22:80" -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-LSK55P2YY4VA4YTL
// 1/3的概率跳转到规则KUBE-SEP-LSK55P2YY4VA4YTL
-A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment "default/nginx-service -> 10.244.2.20:80" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-ONDQTO2GE7J66ITK
//剩下2/3的1/2，即1/3跳转到规则KUBE-SEP-ONDQTO2GE7J66ITK
-A KUBE-SVC-V2OKYYMBY3REGZOG -m comment --comment "default/nginx-service -> 10.244.2.21:80" -j KUBE-SEP-SLTKXEH2UHIG4YH2
//剩下1/3跳转到规则KUBE-SEP-SLTKXEH2UHIG4YH2

[root@k8s-master k8s-yaml]# iptables-save | grep 具体规则
-A KUBE-SEP-LSK55P2YY4VA4YTL -s 10.244.1.22/32 -m comment --comment "default/nginx-service" -j KUBE-MARK-MASQ        
//从pod容器内部访问cluster IP
-A KUBE-SEP-LSK55P2YY4VA4YTL -p tcp -m comment --comment "default/nginx-service" -m tcp -j DNAT --to-destination 10.244.1.22:80

-A KUBE-SEP-ONDQTO2GE7J66ITK -s 10.244.2.20/32 -m comment --comment "default/nginx-service" -j KUBE-MARK-MASQ
-A KUBE-SEP-ONDQTO2GE7J66ITK -p tcp -m comment --comment "default/nginx-service" -m tcp -j DNAT --to-destination 10.244.2.20:80

-A KUBE-SEP-SLTKXEH2UHIG4YH2 -s 10.244.2.21/32 -m comment --comment "default/nginx-service" -j KUBE-MARK-MASQ
-A KUBE-SEP-SLTKXEH2UHIG4YH2 -p tcp -m comment --comment "default/nginx-service" -m tcp -j DNAT --to-destination 10.244.2.21:80


上述规则表明访问service的请求分别转发到后端的三个pod，负载均衡策略
Cluster每一个节点都配置相同的iptables规则，确保整个集群都可以通过Cluster IP访问到Service
```

### 3.2.DNS访问Service

每当新建Service，coreDns会添加该Service的DNS记录，Cluster中的Pod可以通过<SERVICE_NAME>.<NAMESPACE_NAME>访问Service

```shell
[root@k8s-master k8s-yaml]# kubectl get deployment -n kube-system
NAME      READY   UP-TO-DATE   AVAILABLE   AGE
coredns   2/2     2            2           40d
[root@k8s-master k8s-yaml]# kubectl get pod -n kube-system | grep dns
coredns-66f779496c-2l6zq             1/1     Running   18 (19h ago)   40d
coredns-66f779496c-rjq6v             1/1     Running   18 (19h ago)   40d
```

```shell
[root@k8s-master k8s-yaml]# kubectl exec -it  nginx-deployment-7c5ddbdf54-9pf5l bash
root@nginx-deployment-7c5ddbdf54-9pf5l:/# curl nginx-service.default:8080
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

Pod和Service同属一个namespace的话，可以省略namespace_name，通过<SERVICE_NAME>访问Service

```shell
root@nginx-deployment-7c5ddbdf54-9pf5l:/# curl nginx-service:8080
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

```shell
root@nginx-deployment-7c5ddbdf54-9pf5l:/# nslookup nginx-service
;; Got recursion not available from 10.96.0.10
Server:         10.96.0.10
Address:        10.96.0.10#53

Name:   nginx-service.default.svc.cluster.local
Address: 10.100.57.112
;; Got recursion not available from 10.96.0.10
```

nginx-service的完整域名为nginx-service.default.svc.cluster.local

多个资源可以在一个YAML文件中定义，需要用`---`分割

Pod和Service不属一个namespace的话，必须通过<SERVICE_NAME>.<NAMESPACE_NAME>访问Service



### 3.3.外网访问Service

```
Service类型：

ClusterIP:
Service通过Cluster内部的IP对外提供服务，只有Cluster内的节点和Pod可以访问

NodePort:
Service通过Cluster节点的静态端口对外提供服务。Cluster外部可以通过<NodeIP>:<NodePort>访问Service
```

nginx_service_nodeport.yml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service-nodeport
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
  - protocol: TCP
    nodePort: 32000            //node节点上监听的端口
    port: 8090                 //ClusterIP上监听的端口
    targetPort: 80             //Pod上监听的端口
```

相当于在ClusterIP 的Service类型上添加了node节点端口的映射

```shell
[root@k8s-master k8s-yaml]# kubectl get service
NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes               ClusterIP   10.96.0.1       <none>        443/TCP          40d
nginx-service            ClusterIP   10.100.57.112   <none>        8080/TCP         6h18m
nginx-service-nodeport   NodePort    10.103.150.34   <none>        8090:32000/TCP   7s

[root@k8s-master k8s-yaml]# curl 10.103.150.34:8090
<html>
<title>Welcome to nginx!</title>
</html>

[root@k8s-master k8s-yaml]# kubectl get nodes -o wide
NAME         STATUS   ROLES           AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION           CONTAINER-RUNTIME
k8s-master   Ready    control-plane   40d   v1.28.2   192.168.230.128   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://26.1.4
k8s-node1    Ready    <none>          40d   v1.28.2   192.168.230.129   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://26.1.4
k8s-node2    Ready    <none>          40d   v1.28.2   192.168.230.130   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://26.1.4
[root@k8s-master k8s-yaml]# curl 192.168.230.129:32000
<html>
<title>Welcome to nginx!</title>
</html>

//外部可通过<NodeIP>:<NodePort>访问Service，浏览器验证
```

```shell
[root@k8s-master k8s-yaml]# iptables-save | grep SW7F3NCRFKRM23GY
:KUBE-EXT-SW7F3NCRFKRM23GY - [0:0]
:KUBE-SVC-SW7F3NCRFKRM23GY - [0:0]

-A KUBE-EXT-SW7F3NCRFKRM23GY -m comment --comment "masquerade traffic for default/nginx-service-nodeport external destinations" -j KUBE-MARK-MASQ
-A KUBE-EXT-SW7F3NCRFKRM23GY -j KUBE-SVC-SW7F3NCRFKRM23GY
-A KUBE-NODEPORTS -p tcp -m comment --comment "default/nginx-service-nodeport" -m tcp --dport 32000 -j KUBE-EXT-SW7F3NCRFKRM23GY
//NodePort相关iptables规则，请求节点端口32000时,会应用请求到pod端口的KUBE-SVC-SW7F3NCRFKRM23GY规则

-A KUBE-SERVICES -d 10.103.150.34/32 -p tcp -m comment --comment "default/nginx-service-nodeport cluster IP" -m tcp --dport 8090 -j KUBE-SVC-SW7F3NCRFKRM23GY
-A KUBE-SVC-SW7F3NCRFKRM23GY ! -s 10.244.0.0/16 -d 10.103.150.34/32 -p tcp -m comment --comment "default/nginx-service-nodeport cluster IP" -m tcp --dport 8090 -j KUBE-MARK-MASQ
//ClusterIP相关iptables规则，请求ClusterIP端口8090时,会应用请求到pod端口的KUBE-SVC-SW7F3NCRFKRM23GY规则

-A KUBE-SVC-SW7F3NCRFKRM23GY -m comment --comment "default/nginx-service-nodeport -> 
10.244.1.22:80" -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-Z5SU7NYCFI2K7HIR
-A KUBE-SVC-SW7F3NCRFKRM23GY -m comment --comment "default/nginx-service-nodeport -> 10.244.2.20:80" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-AKGUI2FJZHFCP6QL
-A KUBE-SVC-SW7F3NCRFKRM23GY -m comment --comment "default/nginx-service-nodeport -> 10.244.2.21:80" -j KUBE-SEP-MM6NPVJ7O6YETGXF
//Pod相关规则
```

NodePort和ClusterIP在各自端口接收到的请求都会通过iptables转发到Pod的targetPort



## 4.Rolling Update

滚动更新的最大好处是零停机，整个更新过程始终有副本在运行，从而保证业务的连续性。

```shell
kubectl apply -f deployment.yml
kubectl get deployment -o wide
kubectl get replicaset -o wide
kubectl get pod -o wide
```

更新deploymnet.yml 镜像版本，重新apply

```shell
kubectl apply -f deployment.yml
kubectl get deployment -o wide            //镜像版本变化
kubectl get replicaset -o wide            //多条replicaset资源
kubectl get pod -o wide                   //逐个更新pod，创建一个新rs的pod，删除一个老rs的pod
```



回滚

kubectl apply 每次更新应用时，k8s都会记录下当前的配置，保存一个版次 revision;

可以通过deployment.yml 文件中spec.revisionHistoryLimit保留最近n次；

```shell
kubectl apply -f deployment.yml --record        
//--record将当前命令记录到revision记录中的CHANGE-CAUSE列信息，方便回滚知道具体文件、版本

kubectl rollout history deployment deployment_name  
//查看revision历史记录,查看REVISION列版本标识数字,可通过CHANGE-CAUSE列记录导入时的文件、命令，
//查看对应yaml文件内容，所以每次更新最好新建yaml文件对同一deployment资源进行更新

kubectl rollout undo deployment deployment_name --to-revision=num    //回滚到具体版本
```



## 5.Health Check

默认健康检查 ： yaml文件定义的spec.restartPolicy重启策略：always,OnFailure,

Liveness和Readiness探测机制提供更精细的健康检查

```
Liveness探测让用户可以自定义判断容器是否健康的条件
spec.containers.livenessProbe

spec.containers.livenessProbe.initialDelaySeconds  容器启动多少秒后开始执行Liveness探测
spec.containers.livenessProbe.periodSeconds  容器每多少秒执行一次Liveness探测，连续3次执行失败，会杀掉并重启容器
```

```
Readiness探测告诉k8s什么时候可以将容器加入Service负载均衡池中，对外提供服务
spec.containers.readinessProbe

spec.containers.readinessProbe.initialDelaySeconds  容器启动多少秒后开始执行Readiness探测
spec.containers.readinessProbe.periodSeconds  容器每多少秒执行一次Readiness探测，连续3次执行失败，会将容器状态READY设置为不可用
```

```
区别：
用Liveness探测容器是否需要重启以实现自愈
用Readiness探测判断容器是否已经准备好对外提供服务

面对探测失败的处理方式：
Liveness探测是重启容器
Readiness探测是将容器设置为不可用，不接收Service转发的请求
```

```yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  template:
    metadata:
      labels:
        run: web
    spec:
      containers:
      - name: web
        image: myhttpd
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:                  //httpGet探测方法，http请求的返回值在200-400之间则为探测成功
            scheme: HTTP
            path: /healthy
            port: 8080             //探测http://[container_ip]:8080/healthy,探测成功才加入web-svc负载均衡中
          initialDelaySeconds: 10
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: web-svc
spec:
  selector:
    run: web
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 80

```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  replicas: 4
  selector:                     //要选择selector匹配标签
    matchLabels:
      run: app
  template:
    metadata:
      labels:
        run: app
    spec:
      containers:
      - name: app
        image: busybox
        args:
        - /bin/sh
        - -c
        - sleep 10; touch /tmp/healthy; sleep 30000
        readinessProbe:
          exec:
            command:
            - cat
            - /tmp/healthy
          initialDelaySeconds: 10
          periodSeconds: 5
```

```shell
kubectl get deployment 资源  相关参数说明：
DESIRED：期望副本数量
CURRENT：当前副本数量，新副本+旧副本
UP-TO-DATE：完成更新副本数量，即几个新副本
AVAILABLE：当前处于READY状态的副本数量，即几个旧副本

kubectl describe deployment 资源 
滚动更新时显示更新状态
更新时默认参数配置
RollingUpdateStrategy:  25% max unavailable, 25% max surge
max surge 控制滚动更新时副本总数超过DESIRED的上限，向上取整
max unavailable 控制滚动更新时不可用的副本占DESIRED的上限，向下取整

示例：
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  13m   deployment-controller  Scaled up replica set app-757c489674 to 4
  Normal  ScalingReplicaSet  106s  deployment-controller  Scaled up replica set app-5bd8fc758c to 1         //新副本增加1个，max surge，总数4+4*25% = 5个
  Normal  ScalingReplicaSet  106s  deployment-controller  Scaled down replica set app-757c489674 to 3 from 4  //旧副本减少1个，max unavailable ，总数4个
  Normal  ScalingReplicaSet  106s  deployment-controller  Scaled up replica set app-5bd8fc758c to 2 from 1  //旧副本减少1个，新副本新建1个，使总数保持在 4+4*25% = 5个

新副本因为没有通过Readiness检测，后续更新卡住，整体情况为：
Replicas:               4 desired | 2 updated | 5 total | 3 available | 2 unavailable
                        期待4个      更新2个      总数5个    3个可用，旧副本  2个不可用，新副本


[root@k8s-master k8s-yaml]# kubectl rollout history deployment app
deployment.apps/app 
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=app.v1.yml --record=true
2         kubectl apply --filename=app.v2.yml --record=true

[root@k8s-master k8s-yaml]# kubectl rollout undo deployment app --to-revision=1
deployment.apps/app rolled back
```

## 6.数据管理

Volume的生命周期独立于容器



1、emptyDir

最基础的volume类型，emptyDir对于容器来说是持久的，对于Pod则不是。Pod从节点删除时，volume的内容也被删除。emptyDir的生命周期和Pod一致

Pod中的所有容器都可以共享volume，可以指定各自的mount路径

emptyDir.yml：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: producer-consumer
spec:   
  containers:
  - name: producer
    image: busybox
    volumeMounts:
    - mountPath: /producer_dir
      name: shared-volume
    args:
    - /bin/sh
    - -c
    - echo "hello world" > /producer_dir/hello ; sleep 30000

  - name: consumer
    image: busybox
    volumeMounts:
    - mountPath: /consumer_dir
      name: shared-volume
    args:
    - /bin/sh
    - -c
    - cat /consumer_dir/hello ; sleep 30000

  volumes:
  - name: shared-volume
    emptyDir: {}
```

```shell
[root@k8s-master k8s-yaml]# kubectl apply -f emptyDir.yml
[root@k8s-master k8s-yaml]# kubectl logs producer-consumer consumer
hello world
相当于在对应node节点执行docker run -v /producer_dir 和 docker run -v /consumer_dir，临时目录。
在对应node节点执行 “docker inspect 容器名” ，可以发现volume位置相同

 "Mounts": [
            {
                "Type": "bind",
                "Source": "/var/lib/kubelet/pods/d3c43aab-3943-41bd-9f57-2a695b5ea935/volumes/kubernetes.io~empty-dir/shared-volume",
                "Destination": "/consumer_dir",
                "Mode": "",
                "RW": true,
                "Propagation": "rprivate"
            }
            ]
pod不存在，emptyDir也对应消失
```



2、hostPath

将Docker Host文件系统中已经存在的目录mount给Pod的容器；

Pod被销毁，hostPath对应的目录还会被保留，持久性比emptyDir强，但是也增加了pod和node节点的耦合性；

Host主机即node节点崩溃，Pod迁移到其他节点，hostPath则无法访问

`kubectl edit --namespace=kube-system pod kube-apiserver-k8s-master`

```yaml
 volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/pki
      type: DirectoryOrCreate
    name: etc-pki
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
```



3、外部Storage Provider

比如云厂商提供的云硬盘或者分布式存储，不同配置参考云厂商官方文档；

存储和k8s集群分离，保证了数据的持久化；



4、PresistentVolume & PresistentVolumeClaim

PV是外部存储系统中的一块存储空间，PV具有持久性，生命周期独立于Pod;

PVC是对PV的申请，指明存储容量大小和访问模式等信息，k8s会查找并提供满足条件的PV；

例子：

先在master节点安装NFS

```shell
安装nfs和rpc
yum install -y nfs-utils rpcbind 
启动并设置自启动
systemctl start rpcbind
systemctl start nfs-server
systemctl enable rpcbind
systemctl enable nfs-server
查看状态
systemctl status nfs
systemctl status rpcbind
创建nfs目录
mkdir -p /nfsdata
nfs服务端访问权限
vi /etc/exports
/nfsdata *(rw,sync,no_root_squash)
重启nfs和rpc
systemctl restart nfs rpcbind
验证
showmount -e 127.0.0.1
Export list for 127.0.0.1:
/nfsdata *
```

创建PV

nfs-pv1.yml

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mypv1
spec:
  capacity:
    storage: 1Gi           //PV容量
  accessModes:             //共三种模式，ReadWriteOnce 以读写模式mount到单个节点
    - ReadWriteOnce        //ReadOnlyMany 以只读模式mount到多个节点
                           //ReadWriteMany 以读写模式mount到多个节点
  persistentVolumeReclaimPolicy: Recycle      //PV回收策略
                                      //Retain保留PV数据，管理员决定是否手工回收
                                      //Recycle清除PV中的数据，相关于rm -rf /thevolume/*
                                      //Delete删除PV在存储资源对应的存储空间
  storageClassName: nfs      //指定PV的class为nfs,PVC可以指定class申请对应class的PV
  nfs:
    path: /nfsdata/pv1       //指定PV在NFS服务器上对应的目录，需对应创建/nfsdata/pv1目录
    server: 192.168.230.128
```

```shell
[root@k8s-master k8s-yaml]# kubectl get pv
NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
mypv1   1Gi        RWO            Recycle          Available           nfs                     6s
```

创建PVC

nfs-pvc1.yml

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc1
spec:
  accessModes:
    - ReadWriteOnce            //指定访问模式
  resources:
    requests:
      storage: 1Gi             //指定容量
  storageClassName: nfs        //指定class
```

```shell
[root@k8s-master k8s-yaml]# kubectl get pvc
NAME     STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mypvc1   Bound    mypv1    1Gi        RWO            nfs            6s
[root@k8s-master k8s-yaml]# kubectl get pv
NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGE
mypv1   1Gi        RWO            Recycle          Bound    default/mypvc1   nfs                     12m
Bound    default/mypvc1 ：PVC已经Bound到PV
```

现在在Pod中使用存储，Pod使用PVC访问PV资源

Pod-nfs.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod1
spec:
  containers:
  - name: mypod1
    image: busybox
    args:
    - /bin/sh
    - -c
    - sleep 30000
    volumeMounts:
    - mountPath: "/mydata"       //vloume mydata 挂载到容器/mydata目录
      name: mydata
  volumes:
    - name: mydata
      persistentVolumeClaim:
        claimName: mypvc1
```

```
kubectl exec  mypod1  touch /mydata/hello
ll /nfsdata/pv1/
hello文件被创建
```

回收PV

不需要使用PV时，可以通过删除PVC回收PV

回收策略为Recycle，PV状态由Released(解除)，新起Pod清理完PV的数据后，PV状态变为Available(可用)

```shell
[root@k8s-master k8s-yaml]# kubectl delete pvc mypvc1
persistentvolumeclaim "mypvc1" deleted
[root@k8s-master k8s-yaml]# kubectl get pod -o wide
NAME                                READY   STATUS    RESTARTS         AGE     IP            NODE        NOMINATED NODE   READINESS GATES
mypod1                              1/1     Running   0                22m     10.244.1.46        k8s-node1   <none>           <none>
recycler-for-mypv1                  0/1     ContainerCreating   0                3s      <none>        k8s-node1   <none>           <none>
//启动一个新Pod recycler-for-mypv1 清除PV 数据
[root@k8s-master k8s-yaml]# kubectl get pv
NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM            STORAGECLASS   REASON   AGE
mypv1   1Gi        RWO            Recycle          Released   default/mypvc1   nfs                     49m
//mypv1状态为Released，解除与mypvc1的Bound，正在清除数据，此时还不可用
[root@k8s-master k8s-yaml]# kubectl get pv
NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM            STORAGECLASS   REASON   AGE
mypv1   1Gi        RWO            Recycle          Available                   nfs                     49m
//PV数据被清除后，mypv1状态变为Available
```

回收策略为Retain，删除PVC后，PV状态一直都是Released(解除)，不能被其他PVC申请。需要删除并重新创建PV，PV状态变为Available(可用)，数据同时被保留。

上述创建PV的方式为静态供给；

PV动态创建，即没有满足PVC条件的PV，会动态创建PV。`kind : StorageClass` 

StorageClass定义如何创建PV

StorageClass支持Delete和Retain两种回收策略，默认为Delete

PVC申请PV时，只需要指定StorageClass、容量以及访问模式即可

例子：

mysql-pv.yml

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs
  nfs:
    path: /nfsdata/mysql-pv
    server: 192.168.230.128
```

mysql-pvc.yml

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: nfs
```

mysql.yml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  ports:
  - port: 3306
  selector:
    app: mysql

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
spec:
  selector:
    matchLabels:
        app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.6
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pvc
```

```shell
启动mysql镜像容器mysql-client并进入容器，通过客户端访问Service mysql
[root@k8s-master pv]# kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -ppassword
If you don't see a command prompt, try pressing enter.

mysql> 

数据库写入数据，模拟节点宕机，mysql容器会在另一个节点启动，验证PV的数据持久性
```



## 7.Secret & Configmap

Secret会以密文的方式存储数据，避免直接在配置文件中保存敏感信息。Secret以Volume形式mount到Pod

创建Secret

```
方式1:  --from-literal
kubectl create secret generic secret_name --from-literal=username=admin --from-literal=password=123456
每个--from-literal对应一个条目
```

```
方式2:  --from-file
echo -n admin > ./username
echo -n 123456 > ./password
kubectl create secret generic secret_name --from-file=./username --from-file=./password
每个文件内容对应一个信息条目
```

```
方式3:  --from-env-file
cat <<EOF > env.txt
username=admin
password=123456
EOF
kubectl create secret generic secret_name --from-env-file=env.txt
--from-env-file 引用文件每行 Key = Value 对应一个信息条目
```

```
方式4:  通过YAML配置文件生成
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
data:
  username: xxxxxx
  password: xxxxxx

xxxxxx 敏感数据用base64加密
echo -n admin | base64
echo -n password | base64
```

```
kubectl get secret  secret_name                //查看存在secret
kubectl describe secret secret_name            //查看条目的key
kubectl edit secret secret_name                //查看条目的value,value显示为base64加密
反编译
echo -n xxxxxx | base64 --decode
```

Pod可以通过Volume或者环境变量的方式使用Secret

Volume方式

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: busybox
    args:
      - /bin/sh
      - -c
      - sleep 10; touch /tmp/healthy; sleep 30000
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: mysecret 
也可自定义存放数据的文件名
  volumes:
  - name: foo
    secret:
      secretName: mysecret 
      items:
      - key: username
        path: my-group/my-username
      - key: password
        path: my-group/my-password
```

```shell
kubectl exec -it mypod sh
/ # ls /etc/foo/
password  username
/ # cat /etc/foo/password 
123456
/ # cat /etc/foo/username 
admin
```

k8s在指定路径/etc/foo下为每条敏感数据创建一个文件，文件名是数据条目的Key，Value以明文存放在文件中

Secret更新后，容器内数据同步更新

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
data:
  username: YWRtaW4=
  password: YWJjZGVm
```

环境变量方式：

环境变量读取Secret方便，但是无法支撑Secret动态更新，因为环境变量env参数是创建pod时写入的，不支持后续更新





ConfigMap:

对于敏感数据，可以使用Secret；对于非敏感数据，比如应用配置信息，可以使用ConfigMap

ConfigMap创建和使用方式类似Secret，不同之处数据以明文形式存放

创建ConfigMap

```
方式1:  --from-literal
kubectl create configmap configmap_name --from-literal=config1=xxx --from-literal=config2=yyy
每个--from-literal对应一个条目
```

```
方式2:  --from-file
echo -n xxx > ./config1
echo -n yyy > ./config2
kubectl create configmap configmap_name --from-file=./config1 --from-file=./config2
每个文件内容对应一个信息条目
```

```
方式3:  --from-env-file
cat <<EOF > env.txt
config1=xxx
config2=yyy
EOF
kubectl create configmap configmap_name --from-env-file=env.txt
--from-env-file 引用文件每行 Key = Value 对应一个信息条目
```

```
方式4:  通过YAML配置文件生成
apiVersion: v1
kind: ConfigMap
metadata:
  name: myconfigmap
data:
  config1: xxx
  config2: yyy
明文输入，不用加密
```

Pod可以通过Volume或者环境变量的方式使用ConfigMap

Volume方式

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod1
spec:
  containers:
  - name: mypod1
    image: busybox
    args:
      - /bin/sh
      - -c
      - sleep 10; touch /tmp/healthy; sleep 30000
    volumeMounts:
    - name: foo1
      mountPath: "/etc/foo"
      readOnly: true
  volumes:
  - name: foo1
    configMap:
      name: myconfigmap 
```

```shell
kubectl exec -it mypod1 sh
/ # ls /etc/foo/
password  username
/ # cat /etc/foo/config1
xxx
/ # cat /etc/foo/config2
yyy
```

大多数情况下，配置信息都以文件形式提供，常--from-file的方式创建ConfigMap；

多个配置项可--from-file一个文件，pod使用时就只有一个key，查看对应key文件名即可；

读取ConfigMap采用Volume方式，比如给Pod传递日志配置信息；

```
kubectl get configmap configmap_name                //查看存在configmap
kubectl describe configmap configmap_name           //查看条目的key
```



## 8.Helm k8s包管理器

如果只有几个服务组成的应用，单独使用kubectl apply -f 形式部署即可，但针对组成应用服务数量多的时候，文件apply的形式不容易组织和管理，这时使用helm。helm解决如下问题：

更高层次管理服务配置；

将服务作为整体发布，解决服务与服务之间依赖关系；

支持参数化配置和多环境部署；

支持整个应用的回滚，kubectl rollout undo只支持单个deployment的回滚；

支持对部署的应用状态整体进行验证，应用（服务）级别的健康检查；



Helm架构

chart和release

chart是创建一个应用的信息集合，包括各种k8s对象的配置模板、参数定义、依赖关系、文档说明等。chart是应用部署的自包含逻辑单元。

release是chart的运行实例，代表了一个正在运行的应用。当chart安装到k8s集群，就生成一个release。



Helm包管理工具，包就指chart。Helm可以：

从零创建chart；

与存储chart的仓库交互、拉取、保存和更新chart；

在k8s集群中安装和卸载release；

更新、回滚和测试release；



Helm包含两个组件：Helm客户端  Tiller服务器

Helm Client -> Tiller -> k8s API

Helm客户端是终端用户使用的命令行工具，本地开发、管理chart，使用命令在远程k8s安装chart，查看、升级、卸载release；

Tiller服务器运行在k8s集群中，会处理Helm客户端的请求，与k8s API 交互，创建管理release，在k8s中安装chart，查看、升级、卸载release;

Helm客户端负责管理开发chart，Tiller服务器负责创建管理release



安装Helm

```
master节点安装Helm客户端：
curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash

[root@k8s-master ~]# helm version
Client: &version.Version{SemVer:"v2.17.0", GitCommit:"a690bad98af45b015bd3da1a41f6218b1a451dbe", GitTreeState:"clean"}
Error: could not find tiller

helm有很多子命令和参数，为了提高使用命令行效率，通常建议安装helm bash命令补全脚本
helm completion bash > .helmrc
echo 'source .helmrc' >> .bashrc

重新登陆后TAB键可以补全命令和参数
```

```
master节点安装Tiller
helm init
Tiller本身是作为容器化应用运行在k8s集群中
[root@k8s-master ~]# kubectl get svc tiller-deploy --namespace=kube-system
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)     AGE
tiller-deploy   ClusterIP   10.106.44.254   <none>        44134/TCP   3m46s
[root@k8s-master ~]# kubectl get deployment tiller-deploy --namespace=kube-system
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
tiller-deploy   1/1     1            1           4m4s
[root@k8s-master ~]# kubectl get pod --namespace=kube-system
NAME                                 READY   STATUS    RESTARTS         AGE
tiller-deploy-756b857bbb-86ght       1/1     Running   0                4m40s

此时执行helm version可以查看客户端和服务端版本信息
[root@k8s-master ~]# helm version
Client: &version.Version{SemVer:"v2.17.0", GitCommit:"a690bad98af45b015bd3da1a41f6218b1a451dbe", GitTreeState:"clean"}
Server: &version.Version{SemVer:"v2.17.0", GitCommit:"a690bad98af45b015bd3da1a41f6218b1a451dbe", GitTreeState:"clean"}
```



使用Helm

Helm和apt、yum一样管理软件包，有仓库，执行helm search可以看到可安装的chart。

```
helm search              //查看可安装的chart
[root@k8s-master ~]# helm repo list          //查看仓库
NAME    URL                          
stable  https://charts.helm.sh/stable        //官方库
local   http://127.0.0.1:8879/charts         //本地库

helm search 关键字                            //搜索对应chart
helm repo add                                //添加更多仓库
```

```
安装chart前对Tiller服务器升级权限
kubectl create serviceaccount --namespace kube-system tiller
kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'

然后执行helm install <chart_name> 安装chart
helm install stable/mysql

helm list                            //显示已部署的release
helm delete <release_name>           //删除release

[root@k8s-master ~]# helm list
NAME                    REVISION        UPDATED                         STATUS          CHART           APP VERSION     NAMESPACE
peddling-peacock        1               Sat Sep 21 20:47:22 2024        DEPLOYED        mysql-1.6.9     5.7.30          default  
[root@k8s-master ~]# helm delete peddling-peacock
release "peddling-peacock" deleted

删除残留
helm ls --all
helm del --purge <release-name>
```



chart详解

chart 将文件放置在预定义的目录结构中，通常整个 chart 被打成 tar 包，而且标注上版本信息，便于 Helm 部署。

``` 
ls ~/.helm/cache/archive              //部署官方仓库chart后，tar包目录
cd ~/.helm/cache/archive 
tar -zxvf xxx.tgz                     //解压

[root@k8s-master archive]# tree mysql
mysql
├── Chart.yaml                                   //描述 chart 的概要信息，name、version 必填项
├── README.md                                    //Markdown 格式，chart 的使用文档
├── templates                                    //各类 Kubernetes 资源的配置模板都放置在这里。Helm 会将 values.yaml 中的参数值注入到模板中生成标准的 YAML 配置文件。           
│   ├── configurationFiles-configmap.yaml
│   ├── deployment.yaml
│   ├── _helpers.tpl                             //如果存在一些信息多个模板都会用到，则可在 templates/_helpers.tpl 中将其定义为子模板，然后通过 template 函数引用。
│   ├── initializationFiles-configmap.yaml
│   ├── NOTES.txt                                 //chart 的简易使用文档，chart 安装成功后会显示此文档内容
│   ├── pvc.yaml
│   ├── secrets.yaml                              //yaml模版都是Go模板语言引用参数，通过values.yaml灵活定制参数，或者template  引用_helpers.tpl 文件定义的子模板
│   ├── serviceaccount.yaml
│   ├── servicemonitor.yaml
│   ├── svc.yaml
│   └── tests
│       ├── test-configmap.yaml
│       └── test.yaml
└── values.yaml                                   //chart支持在安装时根据参数进行定制化配置，而 values.yaml则提供了这些配置参数的默认值。

chart根目录下其他文件 
LICENSE：文本文件，描述 chart 的许可信息
requirements.yaml ：chart 可能依赖其他的 chart，这些依赖关系可通过 requirements.yaml 指定，安装过程中依赖的chart也会被安装

helm install stable/mysql -n <release_name>      指定release_name
```

实践 chart

```
查看相关准备工作，安装之前需要先清楚 chart 的使用方法。这些信息通常记录在 values.yaml 和 README.md 中。或者执行 helm inspect values <chart-name>
查看得知需先准备相关PV
## Persist data to a persistent volume
persistence:
  enabled: true
  ## database data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  # storageClass: "-"
  accessMode: ReadWriteOnce
  size: 8Gi
  annotations: {}
```

创建pv.yml

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-chart-pv
spec:
  capacity:
    storage: 8Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
#  storageClassName: nfs
  nfs:
    path: /nfsdata/mysql-chart-pv
    server: 192.168.230.128
```

Helm 有两种方式传递配置参数：

1. 指定自己的 values 文件。
   通常的做法是首先通过 `helm inspect values <chart-name> > values.yaml`生成 values 文件，然后设置相关参数`mysqlRootPassword`，之后执行 `helm install --values=values.yaml <chart-name>`。

2. 通过 `--set` 直接传入参数值，比如：

   `helm install <chart-name> --set key=value -n <release-name>`

通过 `helm list` 和 `helm status <release-name>` 可以查看 chart 的最新状态。



升级和回滚release

release 发布后可以执行 `helm upgrade` 对其升级，通过 `--values` 或 `--set`应用新的配置。

`helm upgrade --set key=value <release-name> <chart-name>`

比如将当前的 MySQL 版本升级到 5.7.15：

`helm upgrade --set imageTag=5.7.15 my stable/mysql`

`helm history <release-name>` 可以查看 release 所有的版本。通过 `helm rollback <release-name> <REVISION>` 可以回滚到任何版本。

```
[root@k8s-master pv]# helm history my
REVISION        UPDATED                         STATUS          CHART           APP VERSION     DESCRIPTION     
1               Sun Sep 22 00:45:40 2024        SUPERSEDED      mysql-1.6.9     5.7.30          Install complete
2               Sun Sep 22 00:47:02 2024        SUPERSEDED      mysql-1.6.9     5.7.30          Upgrade complete

[root@k8s-master pv]# helm rollback my 1
Rollback was a success.
```



开发自己的chart

##### 创建 chart

执行 `helm create <chart-name>` 的命令创建 chart ：

`tree <chart-name>` 查看层级目录

新建的chart默认包含一个nginx应用示例。

开发中可以多参考官方 chart 中的模板、values.yaml、Chart.yaml

##### 调试 chart

Helm 提供了 debug 的工具：`helm lint` 和 `helm install --dry-run --debug`。

`helm lint <chart-name> ` 会检测 chart 的语法，报告错误以及给出建议

`helm install --dry-run <chart-name> --debug` 会模拟安装 chart，并输出每个模板生成的 YAML 内容。

##### 安装chart

1. 安装仓库中的 chart，例如：`helm install stable/nginx`
2. 通过 tar 包安装，例如：`helm install ./nginx-1.2.3.tgz`
3. 通过 chart 本地目录安装，例如：`helm install ./nginx`
4. 通过 URL 安装，例如：`helm install https://example.com/charts/nginx-1.2.3.tgz`

##### 搭建chart仓库

```
1、在仓库所在服务器上启动一个httpd容器
 docker run -d --name chart_repo -p 8888:80 -v /var/http/:/usr/local/apache2/htdocs/ httpd
2、通过 helm package <chart-name>将 chart 打包
[root@k8s-master chart]# helm package mychart
Successfully packaged chart and saved it to: /root/k8s-yaml/chart/mychart-0.1.0.tgz
3、执行 helm repo index 生成仓库的 index 文件
mkdir -p myrepo               //创建仓库文件夹
mv xxx.tgz myrepo/            //将所有chart包都移入仓库内
helm repo index myrepo/ --url http://xx.xx.xx.xx:端口/charts       
Helm 会扫描 myrepo 目录中的所有 tgz 包并生成 index.yaml。--url指定的是新仓库的访问路径。新生成的 index.yaml 记录了当前仓库中所有 chart 的信息：
4、将 xxx.tgz 和 index.yaml 上传到 仓库服务器 的 /var/http/charts 目录
 scp xxx root@xx.xx.xx.xx:/var/http/charts
5、通过 helm repo add <repo-name> <url> 将新仓库添加到Helm
helm repo add newrepo http://xx.xx.xx.xx:8888/charts
helm repo list 查看新添加repo
6、现在已经可以 helm search <chart-name> 
[root@k8s-master myrepo]# helm search mychart
NAME            CHART VERSION   APP VERSION     DESCRIPTION                
local/mychart   0.1.0           1.0             A Helm chart for Kubernetes
newrepo/mychart 0.1.0           1.0             A Helm chart for Kubernetes
7、helm install newrepo/mychart
8、如果以后远程chart仓库添加了新的chart并且更新了index.yaml，需要用 helm repo update 更新本地的 index后才能看到私有仓库新增的chart
```

